\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{albawiUnderstandingConvolutionalNeural2017}
\citation{lauerIncorporatingPriorKnowledge2008}
\citation{goodfellowDeepLearning2016}
\citation{hutchisonEvaluationPoolingOperations2010b}
\citation{decosteTrainingInvariantSupport2002}
\citation{scholkopfIncorporatingInvariancesSupport1996}
\citation{lauerIncorporatingPriorKnowledge2008}
\citation{kauderer-abramsQuantifyingTranslationInvarianceConvolutional2017}
\citation{taylorImprovingDeepLearning2017}
\citation{scholkopfIncorporatingInvariancesSupport1996}
\citation{decosteTrainingInvariantSupport2002}
\citation{haasdonkTangentDistanceKernels2002}
\citation{haasdonkInvarianceKernelMethods2005}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Related Work}{1}{section.2}}
\citation{vapnikNatureStatisticalLearning1995}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Two steps are shown of a $4\times 4$ input layer that is filtered with a max-pooling filter of size $2\times 2$ with a stride of 1. Each square of $2\times 2$ in the original layer is merged into a single value in the output layer by taking the maximum value in the $2\times 2 $ filter. The resulting output layer has a reduced size of $3\times 3$.\relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{max-pooling}{{1}{2}{Two steps are shown of a $4\times 4$ input layer that is filtered with a max-pooling filter of size $2\times 2$ with a stride of 1. Each square of $2\times 2$ in the original layer is merged into a single value in the output layer by taking the maximum value in the $2\times 2 $ filter. The resulting output layer has a reduced size of $3\times 3$.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Max-Pooling}{2}{section.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Max-Pooling Kernels}{2}{section.4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Experiments}{2}{section.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Two samples of the artificial data set with left a sample from class 0 and right a sample from class 2.\relax }}{3}{figure.caption.2}}
\newlabel{lines}{{2}{3}{Two samples of the artificial data set with left a sample from class 0 and right a sample from class 2.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}\hskip -1em.\nobreakspace  {}Exp. 1: Are Pooling SVMs Invariant?}{3}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The 5-run averaged errors of a standard RBF SVM, two pooling SVMs and a VSV SVM on the artificial data set for various values of $\gamma $.\relax }}{3}{figure.caption.3}}
\newlabel{fig:results1}{{3}{3}{The 5-run averaged errors of a standard RBF SVM, two pooling SVMs and a VSV SVM on the artificial data set for various values of $\gamma $.\relax }{figure.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The minimum 5-run averaged errors of a standard RBF SVM, two pooling SVMs and a VSV SVM on the artificial data set with the corresponding $\gamma $ value.\relax }}{3}{table.caption.4}}
\newlabel{tab:results1}{{1}{3}{The minimum 5-run averaged errors of a standard RBF SVM, two pooling SVMs and a VSV SVM on the artificial data set with the corresponding $\gamma $ value.\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}\hskip -1em.\nobreakspace  {}Exp. 2: Pooling Kernels and Variable Training Set Sizes}{3}{subsection.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Learning curves of a standard RBF SVM, two pooling SVMs and a VSV SVM on the artificial data set for fixed values of $\gamma $.\relax }}{4}{figure.caption.5}}
\newlabel{fig:learningcurve}{{4}{4}{Learning curves of a standard RBF SVM, two pooling SVMs and a VSV SVM on the artificial data set for fixed values of $\gamma $.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}\hskip -1em.\nobreakspace  {}Exp. 3: Pooling SVMs and Noisy Data}{4}{subsection.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Two samples of the noisy artificial data set with left a sample from class 1 and right a sample from class 2.\relax }}{4}{figure.caption.6}}
\newlabel{lines}{{5}{4}{Two samples of the noisy artificial data set with left a sample from class 1 and right a sample from class 2.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The 5-run averaged errors of a standard RBF SVM, two pooling SVMs and a VSV SVM on the noisy artificial data set for various values of $\gamma $.\relax }}{4}{figure.caption.7}}
\newlabel{fig:resultsnoise}{{6}{4}{The 5-run averaged errors of a standard RBF SVM, two pooling SVMs and a VSV SVM on the noisy artificial data set for various values of $\gamma $.\relax }{figure.caption.7}{}}
\bibstyle{ieee_fullname}
\bibdata{egbib}
\bibcite{albawiUnderstandingConvolutionalNeural2017}{1}
\bibcite{decosteTrainingInvariantSupport2002}{2}
\bibcite{goodfellowDeepLearning2016}{3}
\bibcite{haasdonkTangentDistanceKernels2002}{4}
\bibcite{haasdonkInvarianceKernelMethods2005}{5}
\bibcite{hutchisonEvaluationPoolingOperations2010b}{6}
\bibcite{kauderer-abramsQuantifyingTranslationInvarianceConvolutional2017}{7}
\bibcite{lauerIncorporatingPriorKnowledge2008}{8}
\bibcite{scholkopfIncorporatingInvariancesSupport1996}{9}
\bibcite{taylorImprovingDeepLearning2017}{10}
\bibcite{vapnikNatureStatisticalLearning1995}{11}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The minimum 5-run averaged errors of a standard RBF SVM, two pooling SVMs and a VSV SVM on the noisy artificial data set with the corresponding $\gamma $ value.\relax }}{5}{table.caption.8}}
\newlabel{tab:results2}{{2}{5}{The minimum 5-run averaged errors of a standard RBF SVM, two pooling SVMs and a VSV SVM on the noisy artificial data set with the corresponding $\gamma $ value.\relax }{table.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Learning curves of a standard RBF SVM, two pooling SVMs and a VSV SVM on the noisy artificial data set for fixed values of $\gamma $.\relax }}{5}{figure.caption.9}}
\newlabel{fig:learningcurves2}{{7}{5}{Learning curves of a standard RBF SVM, two pooling SVMs and a VSV SVM on the noisy artificial data set for fixed values of $\gamma $.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}\hskip -1em.\nobreakspace  {}Discussion}{5}{section.6}}

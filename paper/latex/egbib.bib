
@inproceedings{albawiUnderstandingConvolutionalNeural2017,
  title = {Understanding of a Convolutional Neural Network},
  booktitle = {2017 {{International Conference}} on {{Engineering}} and {{Technology}} ({{ICET}})},
  author = {Albawi, S. and Mohammed, T. A. and {Al-Zawi}, S.},
  year = {2017},
  month = aug,
  pages = {1--6},
  doi = {10.1109/ICEngTechnol.2017.8308186},
  abstract = {The term Deep Learning or Deep Neural Network refers to Artificial Neural Networks (ANN) with multi layers. Over the last few decades, it has been considered to be one of the most powerful tools, and has become very popular in the literature as it is able to handle a huge amount of data. The interest in having deeper hidden layers has recently begun to surpass classical methods performance in different fields; especially in pattern recognition. One of the most popular deep neural networks is the Convolutional Neural Network (CNN). It take this name from mathematical linear operation between matrixes called convolution. CNN have multiple layers; including convolutional layer, non-linearity layer, pooling layer and fully-connected layer. The convolutional and fully-connected layers have parameters but pooling and non-linearity layers don't have parameters. The CNN has an excellent performance in machine learning problems. Specially the applications that deal with image data, such as largest image classification data set (Image Net), computer vision, and in natural language processing (NLP) and the results achieved were very amazing. In this paper we will explain and define all the elements and important issues related to CNN, and how these elements work. In addition, we will also state the parameters that effect CNN efficiency. This paper assumes that the readers have adequate knowledge about both machine learning and artificial neural network.},
  file = {/Users/tuhindas/Zotero/storage/FQA7XI83/8308186.html},
  keywords = {artificial neural network,artificial neural networks,Artificial Neural Networks,classical methods performance,CNN,computer vision,Convolution,convolutional connected layers,convolutional neural network,convolutional neural networks,Convolutional neural networks,deep learning,Deep Neural Network,deeper hidden layers,Feature extraction,feedforward neural nets,fully-connected layers,image classification,image data,Image edge detection,Image recognition,largest image classification data,learning (artificial intelligence),machine learning,mathematical linear operation,matrixes called convolution,multilayers,multiple layers,natural language processing,Neurons,nonlinearity layer,pooling,term Deep Learning}
}

@inproceedings{boserTrainingAlgorithmOptimal1992,
  title = {A Training Algorithm for Optimal Margin Classifiers},
  booktitle = {Proceedings of the Fifth Annual Workshop on {{Computational}} Learning Theory},
  author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
  year = {1992},
  month = jul,
  pages = {144--152},
  publisher = {{Association for Computing Machinery}},
  address = {{Pittsburgh, Pennsylvania, USA}},
  doi = {10.1145/130385.130401},
  abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
  file = {/Users/tuhindas/Zotero/storage/2YAI6S57/Boser et al. - 1992 - A training algorithm for optimal margin classifier.pdf},
  isbn = {978-0-89791-497-0},
  series = {{{COLT}} '92}
}

@article{burgesTutorialSupportVector1998,
  title = {A {{Tutorial}} on {{Support Vector Machines}} for {{Pattern Recognition}}},
  author = {Burges, Christopher J.C.},
  year = {1998},
  month = jun,
  volume = {2},
  pages = {121--167},
  issn = {1573-756X},
  doi = {10.1023/A:1009715923555},
  abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
  file = {/Users/tuhindas/Zotero/storage/A4BTRZS5/Burges - 1998 - A Tutorial on Support Vector Machines for Pattern .pdf},
  journal = {Data Mining and Knowledge Discovery},
  language = {en},
  number = {2}
}

@article{decosteTrainingInvariantSupport2002,
  title = {Training {{Invariant Support Vector Machines}}},
  author = {Decoste, Dennis and Sch{\"o}lkopf, Bernhard},
  year = {2002},
  month = jan,
  volume = {46},
  pages = {161--190},
  issn = {1573-0565},
  doi = {10.1023/A:1012454411458},
  abstract = {Practical experience has shown that in order to obtain the best possible performance, prior knowledge about invariances of a classification problem at hand ought to be incorporated into the training procedure. We describe and review all known methods for doing so in support vector machines, provide experimental results, and discuss their respective merits. One of the significant new results reported in this work is our recent achievement of the lowest reported test error on the well-known MNIST digit recognition benchmark task, with SVM training times that are also significantly faster than previous SVM methods.},
  file = {/Users/tuhindas/Zotero/storage/WSWFU3FV/Decoste and Schölkopf - 2002 - Training Invariant Support Vector Machines.pdf},
  journal = {Machine Learning},
  language = {en},
  number = {1}
}

@inproceedings{dediegoCombiningKernelInformation2004,
  title = {Combining {{Kernel Information}} for {{Support Vector Classification}}},
  booktitle = {Multiple {{Classifier Systems}}},
  author = {{de Diego}, Isaac Mart{\'i}n and Moguerza, Javier M. and Mu{\~n}oz, Alberto},
  editor = {Roli, Fabio and Kittler, Josef and Windeatt, Terry},
  year = {2004},
  pages = {102--111},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-25966-4_10},
  abstract = {In this paper we describe new methods to built a kernel matrix from a collection of kernels for classification purposes using Support Vector Machines (SVMs). The methods build the combination by quantifying, relative to the classification labels, the difference of information among the kernels. The proposed techniques have been successfully evaluated on a variety of artificial and real data sets.},
  file = {/Users/tuhindas/Zotero/storage/BDPI64DE/de Diego et al. - 2004 - Combining Kernel Information for Support Vector Cl.pdf},
  isbn = {978-3-540-25966-4},
  keywords = {Handwritten Digit,Kernel Matrix,Multivariate Adaptative Regression Spline,Support Vector,Support Vector Machine},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{dietterichIncorporatingInvariancesNonlinear2002a,
  title = {Incorporating {{Invariances}} in {{Nonlinear Support Vector Machines}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 14},
  editor = {Dietterich, Thomas G. and Becker, Suzanna and Ghahramani, Zoubin},
  year = {2002},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/1120.003.0083},
  abstract = {The choice of an SVM kernel corresponds to the choice of a representation of the data in a feature space and, to improve performance , it should therefore incorporate prior knowledge such as known transformation invariances. We propose a technique which extends earlier work and aims at incorporating invariances in nonlinear kernels. We show on a digit recognition task that the proposed approach is superior to the Virtual Support Vector method, which previously had been the method of choice.},
  file = {/Users/tuhindas/Zotero/storage/BM8F4IGY/Dietterich et al. - 2002 - Incorporating Invariances in Nonlinear Support Vec.pdf},
  isbn = {978-0-262-27173-8},
  language = {en}
}

@article{duvenaudAutomaticModelConstruction,
  title = {Automatic {{Model Construction}}  with {{Gaussian Processes}}},
  author = {Duvenaud, David Kristjanson},
  pages = {157},
  file = {/Users/tuhindas/Zotero/storage/7XIL2NLZ/Duvenaud - Automatic Model Construction  with Gaussian Proces.pdf},
  language = {en}
}

@article{gonenMultipleKernelLearninga,
  title = {Multiple {{Kernel Learning Algorithms}}},
  author = {Gonen, Mehmet and Alpayd{\i}n, Ethem and Tr, Boun Edu and Tr, Boun Edu},
  pages = {58},
  abstract = {In recent years, several methods have been proposed to combine multiple kernels instead of using a single one. These different kernels may correspond to using different notions of similarity or may be using information coming from multiple sources (different representations or different feature subsets). In trying to organize and highlight the similarities and differences between them, we give a taxonomy of and review several multiple kernel learning algorithms. We perform experiments on real data sets for better illustration and comparison of existing algorithms. We see that though there may not be large differences in terms of accuracy, there is difference between them in complexity as given by the number of stored support vectors, the sparsity of the solution as given by the number of used kernels, and training time complexity. We see that overall, using multiple kernels instead of a single one is useful and believe that combining kernels in a nonlinear or data-dependent way seems more promising than linear combination in fusing information provided by simple linear kernels, whereas linear methods are more reasonable when combining complex Gaussian kernels.},
  file = {/Users/tuhindas/Zotero/storage/6PEW6XBV/Gonen et al. - Multiple Kernel Learning Algorithms.pdf},
  language = {en}
}

@book{goodfellowDeepLearning2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT Press}}
}

@article{grettonIntroductionRKHSSimple,
  title = {Introduction to {{RKHS}}, and Some Simple Kernel Algorithms},
  author = {Gretton, Arthur},
  pages = {33},
  file = {/Users/tuhindas/Zotero/storage/IRKEJPZ8/Gretton - Introduction to RKHS, and some simple kernel algor.pdf},
  language = {en}
}

@inproceedings{haasdonkInvarianceKernelMethods2005,
  title = {Invariance in {{Kernel Methods}} by {{Haar}}-{{Integration Kernels}}},
  booktitle = {Image {{Analysis}}},
  author = {Haasdonk, B. and Vossen, A. and Burkhardt, H.},
  editor = {Kalviainen, Heikki and Parkkinen, Jussi and Kaarna, Arto},
  year = {2005},
  pages = {841--851},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11499145_85},
  abstract = {We address the problem of incorporating transformation invariance in kernels for pattern analysis with kernel methods. We introduce a new class of kernels by so called Haar-integration over transformations. This results in kernel functions, which are positive definite, have adjustable invariance, can capture simultaneously various continuous or discrete transformations and are applicable in various kernel methods. We demonstrate these properties on toy examples and experimentally investigate the real-world applicability on an image recognition task with support vector machines. For certain transformations remarkable complexity reduction is demonstrated. The kernels hereby achieve state-of-the-art results, while omitting drawbacks of existing methods.},
  file = {/Users/tuhindas/Zotero/storage/2TZFERZQ/Haasdonk et al. - 2005 - Invariance in Kernel Methods by Haar-Integration K.pdf},
  isbn = {978-3-540-31566-7},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{haasdonkTangentDistanceKernels2002,
  title = {Tangent Distance Kernels for Support Vector Machines},
  booktitle = {Object Recognition Supported by User Interaction for Service Robots},
  author = {Haasdonk, B. and Keysers, D.},
  year = {2002},
  month = aug,
  volume = {2},
  pages = {864-868 vol.2},
  issn = {1051-4651},
  doi = {10.1109/ICPR.2002.1048439},
  abstract = {When dealing with pattern recognition problems one encounters different types of a-priori knowledge. It is important to incorporate such knowledge into the classification method at hand. A very common type of a-priori knowledge is transformation invariance of the input data, e.g. geometric transformations of image-data like shifts, scaling etc. Distance based classification methods can make use of this by a modified distance measure called tangent distance. We introduce a new class of kernels for support vector machines which incorporate tangent distance and therefore are applicable in cases where such transformation invariances are known. We report experimental results which show that the performance of our method is comparable to other state-of-the-art methods, while problems of existing ones are avoided.},
  file = {/Users/tuhindas/Zotero/storage/8HKXDWP5/Haasdonk and Keysers - 2002 - Tangent distance kernels for support vector machin.pdf;/Users/tuhindas/Zotero/storage/7RCZY7TT/1048439.html;/Users/tuhindas/Zotero/storage/RKMCLIMR/1048439.html},
  keywords = {Classification algorithms,Computer science,decision theory,Design methodology,distance based classification methods,geometric transformations,image-data like shifts,Kernel,learning automata,Machine learning,Marine vehicles,modified distance measure,Optical character recognition software,pattern classification,pattern recognition,scaling,Support vector machine classification,support vector machines,Support vector machines,tangent distance kernels,Training data,transformation invariance}
}

@incollection{hutchisonEvaluationPoolingOperations2010b,
  title = {Evaluation of {{Pooling Operations}} in {{Convolutional Architectures}} for {{Object Recognition}}},
  booktitle = {Artificial {{Neural Networks}} \textendash{} {{ICANN}} 2010},
  author = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Scherer, Dominik and M{\"u}ller, Andreas and Behnke, Sven},
  editor = {Diamantaras, Konstantinos and Duch, Wlodek and Iliadis, Lazaros S.},
  year = {2010},
  volume = {6354},
  pages = {92--101},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-15825-4_10},
  abstract = {A common practice to gain invariant features in object recognition models is to aggregate multiple low-level features over a small neighborhood. However, the differences between those models makes a comparison of the properties of different aggregation functions hard. Our aim is to gain insight into different functions by directly comparing them on a fixed architecture for several common object recognition tasks. Empirical results show that a maximum pooling operation significantly outperforms subsampling operations. Despite their shift-invariant properties, overlapping pooling windows are no significant improvement over non-overlapping pooling windows. By applying this knowledge, we achieve state-of-the-art error rates of 4.57\% on the NORB normalized-uniform dataset and 5.6\% on the NORB jittered-cluttered dataset.},
  file = {/Users/tuhindas/Zotero/storage/PSYW7826/Hutchison et al. - 2010 - Evaluation of Pooling Operations in Convolutional .pdf},
  isbn = {978-3-642-15824-7 978-3-642-15825-4},
  language = {en}
}

@article{kauderer-abramsQuantifyingTranslationInvarianceConvolutional2017,
  title = {Quantifying {{Translation}}-{{Invariance}} in {{Convolutional Neural Networks}}},
  author = {{Kauderer-Abrams}, Eric},
  year = {2017},
  month = dec,
  abstract = {A fundamental problem in object recognition is the development of image representations that are invariant to common transformations such as translation, rotation, and small deformations. There are multiple hypotheses regarding the source of translation invariance in CNNs. One idea is that translation invariance is due to the increasing receptive field size of neurons in successive convolution layers. Another possibility is that invariance is due to the pooling operation. We develop a simple a tool, the translation-sensitivity map, which we use to visualize and quantify the translation-invariance of various architectures. We obtain the surprising result that architectural choices such as the number of pooling layers and the convolution filter size have only a secondary effect on the translation-invariance of a network. Our analysis identifies training data augmentation as the most important factor in obtaining translation-invariant representations of images using convolutional neural networks.},
  archiveprefix = {arXiv},
  eprint = {1801.01450},
  eprinttype = {arxiv},
  file = {/Users/tuhindas/Zotero/storage/ZUCBKR95/Kauderer-Abrams - 2017 - Quantifying Translation-Invariance in Convolutiona.pdf;/Users/tuhindas/Zotero/storage/NWXDMW7J/1801.html},
  journal = {arXiv:1801.01450 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass = {cs}
}

@article{lauerIncorporatingPriorKnowledge2008,
  title = {Incorporating Prior Knowledge in Support Vector Machines for Classification: {{A}} Review},
  shorttitle = {Incorporating Prior Knowledge in Support Vector Machines for Classification},
  author = {Lauer, Fabien and Bloch, G{\'e}rard},
  year = {2008},
  month = mar,
  volume = {71},
  pages = {1578--1594},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2007.04.010},
  abstract = {For classification, support vector machines (SVMs) have recently been introduced and quickly became the state of the art. Now, the incorporation of prior knowledge into SVMs is the key element that allows to increase the performance in many applications. This paper gives a review of the current state of research regarding the incorporation of two general types of prior knowledge into SVMs for classification. The particular forms of prior knowledge considered here are presented in two main groups: class-invariance and knowledge on the data. The first one includes invariances to transformations, to permutations and in domains of input space, whereas the second one contains knowledge on unlabeled data, the imbalance of the training set or the quality of the data. The methods are then described and classified into the three categories that have been used in literature: sample methods based on the modification of the training data, kernel methods based on the modification of the kernel and optimization methods based on the modification of the problem formulation. A recent method, developed for support vector regression, considers prior knowledge on arbitrary regions of the input space. It is exposed here when applied to the classification case. A discussion is then conducted to regroup sample and optimization methods under a regularization framework.},
  file = {/Users/tuhindas/Zotero/storage/PTRYEBYX/Lauer and Bloch - 2008 - Incorporating prior knowledge in support vector ma.pdf;/Users/tuhindas/Zotero/storage/74ZPHQM8/S0925231207001439.html},
  journal = {Neurocomputing},
  keywords = {Classification,Invariance,Pattern recognition,Prior knowledge,Support vector machine (SVM)},
  language = {en},
  number = {7},
  series = {Progress in {{Modeling}}, {{Theory}}, and {{Application}} of {{Computational Intelligenc}}}
}

@misc{LecturePartCSE3130,
  title = {Lecture Part 1 - {{CSE3130 Introduction}} to {{Quantum Computer Science}} (2020/21 {{Q3}})},
  howpublished = {https://brightspace.tudelft.nl/d2l/le/content/281795/viewContent/2089199/View}
}

@article{niuNovelHybridCNN2012,
  title = {A Novel Hybrid {{CNN}}\textendash{{SVM}} Classifier for Recognizing Handwritten Digits},
  author = {Niu, Xiao-Xiao and Suen, Ching Y.},
  year = {2012},
  month = apr,
  volume = {45},
  pages = {1318--1325},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2011.09.021},
  abstract = {This paper presents a hybrid model of integrating the synergy of two superior classifiers: Convolutional Neural Network (CNN) and Support Vector Machine (SVM), which have proven results in recognizing different types of patterns. In this model, CNN works as a trainable feature extractor and SVM performs as a recognizer. This hybrid model automatically extracts features from the raw images and generates the predictions. Experiments have been conducted on the well-known MNIST digit database. Comparisons with other studies on the same database indicate that this fusion has achieved better results: a recognition rate of 99.81\% without rejection, and a recognition rate of 94.40\% with 5.60\% rejection. These performances have been analyzed with reference to those by human subjects.},
  file = {/Users/tuhindas/Zotero/storage/7AUCYMAW/Niu and Suen - 2012 - A novel hybrid CNN–SVM classifier for recognizing .pdf;/Users/tuhindas/Zotero/storage/G9S8KHZF/S0031320311004006.html},
  journal = {Pattern Recognition},
  keywords = {Convolutional Neural Network,Handwritten digit recognition,Hybrid model,Support Vector Machine},
  language = {en},
  number = {4}
}

@book{paulsenIntroductionTheoryReproducing2016a,
  title = {An {{Introduction}} to the {{Theory}} of {{Reproducing Kernel Hilbert Spaces}}},
  author = {Paulsen, Vern I. and Raghupathi, Mrinal},
  year = {2016},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9781316219232},
  abstract = {These notes give an introduction to the theory of reproducing kernel Hilbert spaces and their multipliers. We begin with the material that is contained in Aronszajn's classic paper on the subject. We take a somewhat algebraic view of some of his results and discuss them in the context of pull-back and push-out constructions. We then prove Schoenberg's results on negative definite functions and his characterization of metric spaces that can be embedded isometrically in Hilbert spaces. Following this we study multipliers of reproducing kernel Hilbert spaces and use them to give classic proofs of Pick's and Nevanlinna's theorems. In later chapters we consider generalizations of this theory to the vector-valued setting.},
  file = {/Users/tuhindas/Zotero/storage/MT7P3BB6/Paulsen and Raghupathi - 2016 - An Introduction to the Theory of Reproducing Kerne.pdf},
  isbn = {978-1-316-21923-2},
  language = {en}
}

@article{pedregosaScikitlearnMachineLearning2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  shorttitle = {Scikit-Learn},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  year = {2011},
  month = nov,
  volume = {12},
  pages = {2825--2830},
  issn = {1532-4435},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  file = {/Users/tuhindas/Zotero/storage/W3AYYP9G/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf},
  journal = {The Journal of Machine Learning Research},
  number = {null}
}

@inproceedings{scholkopfIncorporatingInvariancesSupport1996,
  title = {Incorporating Invariances in Support Vector Learning Machines},
  booktitle = {Artificial {{Neural Networks}} \textemdash{} {{ICANN}} 96},
  author = {Sch{\"o}lkopf, Bernhard and Burges, Chris and Vapnik, Vladimir},
  editor = {{von der Malsburg}, Christoph and {von Seelen}, Werner and Vorbr{\"u}ggen, Jan C. and Sendhoff, Bernhard},
  year = {1996},
  pages = {47--52},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-61510-5_12},
  abstract = {Developed only recently, support vector learning machines achieve high generalization ability by minimizing a bound on the expected test error; however, so far there existed no way of adding knowledge about invariances of a classification problem at hand. We present a method of incorporating prior knowledge about transformation invariances by applying transformations to support vectors, the training examples most critical for determining the classification boundary.},
  file = {/Users/tuhindas/Zotero/storage/7LZVP64N/Schölkopf et al. - 1996 - Incorporating invariances in support vector learni.pdf},
  isbn = {978-3-540-68684-2},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{scholkopfLearningKernelsSupport2001,
  title = {Learning with {{Kernels}}: {{Support Vector Machines}}, {{Regularization}}, {{Optimization}}, and {{Beyond}}},
  shorttitle = {Learning with {{Kernels}}},
  author = {Scholkopf, Bernhard and Smola, Alexander J.},
  year = {2001},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  abstract = {From the Publisher: In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs -kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.},
  isbn = {978-0-262-19475-4}
}

@article{springenbergStrivingSimplicityAll2015,
  title = {Striving for {{Simplicity}}: {{The All Convolutional Net}}},
  shorttitle = {Striving for {{Simplicity}}},
  author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  year = {2015},
  month = apr,
  abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding \textendash{} and building on other recent work for finding simple network structures \textendash{} we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the ``deconvolution approach'' for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
  archiveprefix = {arXiv},
  eprint = {1412.6806},
  eprinttype = {arxiv},
  file = {/Users/tuhindas/Zotero/storage/AE77KTZ3/Springenberg et al. - 2015 - Striving for Simplicity The All Convolutional Net.pdf},
  journal = {arXiv:1412.6806 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{taylorImprovingDeepLearning2017,
  title = {Improving {{Deep Learning}} Using {{Generic Data Augmentation}}},
  author = {Taylor, Luke and Nitschke, Geoff},
  year = {2017},
  month = aug,
  abstract = {Deep artificial neural networks require a large corpus of training data in order to effectively learn, where collection of such training data is often expensive and laborious. Data augmentation overcomes this issue by artificially inflating the training set with label preserving transformations. Recently there has been extensive use of generic data augmentation to improve Convolutional Neural Network (CNN) task performance. This study benchmarks various popular data augmentation schemes to allow researchers to make informed decisions as to which training methods are most appropriate for their data sets. Various geometric and photometric schemes are evaluated on a coarse-grained data set using a relatively simple CNN. Experimental results, run using 4-fold cross-validation and reported in terms of Top-1 and Top-5 accuracy, indicate that cropping in geometric augmentation significantly increases CNN task performance.},
  archiveprefix = {arXiv},
  eprint = {1708.06020},
  eprinttype = {arxiv},
  file = {/Users/tuhindas/Zotero/storage/AHN7ZTSV/Taylor and Nitschke - 2017 - Improving Deep Learning using Generic Data Augment.pdf;/Users/tuhindas/Zotero/storage/WDFL6KRK/1708.html},
  journal = {arXiv:1708.06020 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@book{vapnikNatureStatisticalLearning1995,
  title = {The {{Nature}} of {{Statistical Learning Theory}}},
  author = {Vapnik, Vladimir N.},
  year = {1995},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-1-4757-2440-0},
  abstract = {The aim of this book is to discuss the fundamental ideas which lie behind the statistical theory of learning and generalization. It considers learning from the general point of view of function estimation based on empirical data. Omitting proofs and technical details, the author concentrates on discussing the main results of learning theory and their connections to fundamental problems in statistics. These include: - the general setting of learning problems and the general model of minimizing the risk functional from empirical data - a comprehensive analysis of the empirical risk minimization principle and shows how this allows for the construction of necessary and sufficient conditions for consistency - non-asymptotic bounds for the risk achieved using the empirical risk minimization principle - principles for controlling the generalization ability of learning machines using small sample sizes - introducing a new type of universal learning machine that controls the generalization ability.},
  file = {/Users/tuhindas/Zotero/storage/QIJMQU8G/Vapnik - 1995 - The Nature of Statistical Learning Theory.pdf;/Users/tuhindas/Zotero/storage/TYAGYLFU/9781475724400.html},
  isbn = {978-1-4757-2440-0},
  language = {en}
}


